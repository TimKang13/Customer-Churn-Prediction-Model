{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-processing given dataset**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "icN7q6rAzsj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to upload history.csv and test.csv to run this notebook"
      ],
      "metadata": {
        "id": "7P6RkCNeHm-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ],
      "metadata": {
        "id": "a4m3GZnHzs9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data!"
      ],
      "metadata": {
        "id": "gYpqKm-7z3SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"history.csv\")\n",
        "df.info()"
      ],
      "metadata": {
        "id": "68Cy5yE4z17H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "19a4e181-f24d-4eb4-d278-5c728d708541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'history.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0a67c796ebbc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"history.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'history.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "spg145k68ZNn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l5FCCTp_8Z67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display some data!"
      ],
      "metadata": {
        "id": "PPDf1-rq0Bs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CBQv_DtRFGG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(20)"
      ],
      "metadata": {
        "id": "fQ82-8BL0EC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider categorical data using one-hot encoding.\n",
        "\n"
      ],
      "metadata": {
        "id": "yU6-ydXr0Fd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_columns = []\n",
        "non_numeric_columns = []\n",
        "for columns in df.columns:\n",
        "    if df[columns].dtype in [\"float64\", \"int64\", \"int8\"]:\n",
        "        numeric_columns.append(columns)\n",
        "    else:\n",
        "        non_numeric_columns.append(columns)\n",
        "\n",
        "for column in non_numeric_columns:\n",
        "    df[column] = df[column].astype('category')\n",
        "    categories = df[column].cat.categories\n",
        "    codes = df[column].cat.codes\n",
        "    cat_to_code = {}\n",
        "    code_to_cat = {}\n",
        "    for i in range(len(categories)):\n",
        "        cat = categories[i]\n",
        "        cat_to_code[cat] = i\n",
        "        code_to_cat[i] = cat\n",
        "    df[column] = codes\n",
        "\n",
        "df.head(20)"
      ],
      "metadata": {
        "id": "PbpXTkMg0Kwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider missing data by assuming the most likely option."
      ],
      "metadata": {
        "id": "mUbga0hz0OXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.copy()\n",
        "\n",
        "\n",
        "for columnb in df.columns:\n",
        "    if len(df[columnb].dropna()) == 0:\n",
        "        most_common = -1\n",
        "    else: most_common = df[columnb].dropna().mode()[0]\n",
        "    data[columnb].fillna(value=most_common, inplace=True)\n",
        "\n",
        "data.head(20)"
      ],
      "metadata": {
        "id": "zexo854E0Rld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the imbalanced data by oversampling."
      ],
      "metadata": {
        "id": "PUG6w4nF0W8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oversampler = RandomOverSampler(sampling_strategy=\"minority\")\n",
        "oversampled_data, oversampled_target = oversampler.fit_resample(data, pd.Series(data[\"label\"]))\n",
        "oversampled_data.shape\n",
        "\n",
        "oversampled_data.head(20)"
      ],
      "metadata": {
        "id": "dSlSYKdW0VWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving our new data!"
      ],
      "metadata": {
        "id": "HDtWw7o_0bUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oversampled_data.to_csv(\"./processed_data.csv\")"
      ],
      "metadata": {
        "id": "ahOqHQoO0d6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LETS PROCESS TEST TOO!!!**"
      ],
      "metadata": {
        "id": "-nAk00Wy6_uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"test.csv\")\n",
        "df.info()\n",
        "\n",
        "numeric_columns = []\n",
        "non_numeric_columns = []\n",
        "for columns in df.columns:\n",
        "    if df[columns].dtype in [\"float64\", \"int64\", \"int8\"]:\n",
        "        numeric_columns.append(columns)\n",
        "    else:\n",
        "        non_numeric_columns.append(columns)\n",
        "\n",
        "for column in non_numeric_columns:\n",
        "    df[column] = df[column].astype('category')\n",
        "    categories = df[column].cat.categories\n",
        "    codes = df[column].cat.codes\n",
        "    cat_to_code = {}\n",
        "    code_to_cat = {}\n",
        "    for i in range(len(categories)):\n",
        "        cat = categories[i]\n",
        "        cat_to_code[cat] = i\n",
        "        code_to_cat[i] = cat\n",
        "    df[column] = codes\n",
        "\n",
        "data = df.copy()\n",
        "\n",
        "for columnb in df.columns:\n",
        "    if len(df[columnb].dropna()) == 0:\n",
        "        most_common = -1\n",
        "    else: most_common = df[columnb].dropna().mode()[0]\n",
        "    data[columnb].fillna(value=most_common, inplace=True)\n",
        "\n",
        "data.to_csv(\"./processed_test.csv\")"
      ],
      "metadata": {
        "id": "BpMvN1X37Cun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CLASSICAL ML**"
      ],
      "metadata": {
        "id": "HcFk-WVU0ymX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze customer behavior within the wealth management services domain. The dataset, consisting of many columns, aggregates information from various tables, offering insights into customer interactions, financial transactions, trading activities, and overall financial behaviors. The focus is on predicting customer churn and estimating lifetime value."
      ],
      "metadata": {
        "id": "VUMv8d0y0ybt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "huZLX02b08K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"processed_data.csv\")"
      ],
      "metadata": {
        "id": "UnqIzeOY1Cje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "vJ-L5BGK1Ece"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = df[df['label'] == 1]\n",
        "filtered_df"
      ],
      "metadata": {
        "id": "3-G6QWGn1Fy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"label\"], axis=1)\n",
        "y = df[\"label\"]"
      ],
      "metadata": {
        "id": "UipsZEAC1IS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#train test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "Dte_EfMo1Ji_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try different classifying algorithms"
      ],
      "metadata": {
        "id": "QwDhCEMO_6IS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = DecisionTreeClassifier()\n",
        "classifier_cv_scores = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=5)\n",
        "print(f\"Average Decision Tree CV Score: {sum(classifier_cv_scores)/len(classifier_cv_scores)}\")\n",
        "\n",
        "classifier.fit(X_train, y_train) # decision tree\n",
        "y_pred = classifier.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "4w_lUHYN1KG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = AdaBoostClassifier(n_estimators=100, learning_rate = 1, random_state=42)\n",
        "clf.fit(X_train, y_train) # Adaboost\n",
        "y_pred = clf.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "InuBlZXy1TL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbc = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1, max_depth = None)\n",
        "gbc.fit(X_train, y_train) # Gradient Boost\n",
        "y_pred = gbc.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "ToATzGoW1U84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors = 8)\n",
        "knn_cv_scores = cross_val_score(estimator=knn, X=X_train, y=y_train, cv=5)\n",
        "print(f\"Average KNN CV Score: {sum(knn_cv_scores)/len(knn_cv_scores)}\")\n",
        "\n",
        "knn.fit(X_train, y_train) # KNN\n",
        "y_pred = knn.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "1mZY-SQP1WZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgc = xgb.XGBClassifier(n_estimators=100, learning_rate = 1, booster = \"gbtree\", random_state = 42, max_depth = 13, max_samples = 0.9)\n",
        "xgc.fit(X_train, y_train) # XGBoost\n",
        "y_pred = xgc.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "YHBEmVrw1YOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logreg_model = LogisticRegression(random_state=42)\n",
        "logreg_model.fit(X_train, y_train) # logistic regression\n",
        "y_pred = logreg_model.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "XusB44AK1eTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc = SVC(kernel = \"rbf\", class_weight = \"balanced\", gamma = \"scale\")\n",
        "svc.fit(X_train, y_train) # SVM\n",
        "y_pred = svc.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "rvtFLlrq1g0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(X_train, y_train) # random forest\n",
        "y_pred = rfc.predict(X_val)\n",
        "f1 = f1_score(y_val, y_pred)\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "Ay5PuFxN1kLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "rfc.fit(X_train, y_train) # random forest"
      ],
      "metadata": {
        "id": "_1lKLlLJ1x9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = rfc.feature_importances_\n",
        "print(\"Feature Importances:\", feature_importances)"
      ],
      "metadata": {
        "id": "0_2R0H-J1ztA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming feature_importances is your array of feature importances\n",
        "top_indices = np.argsort(feature_importances)[::-1][:5]  # Get the indices of top 5 features\n",
        "\n",
        "# Print the top 5 features and their importances\n",
        "print(\"Top 5 Features:\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"Feature {i+1}: Index={idx}, Importance={feature_importances[idx]}\")"
      ],
      "metadata": {
        "id": "j2RUe4x211XX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_feature_columns = df.iloc[:, top_indices]\n",
        "combined_df = pd.concat([top_feature_columns, df['label']], axis=1)\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "display(combined_df.head(100))"
      ],
      "metadata": {
        "id": "k53-jQGo133m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = combined_df.groupby('label')\n",
        "median_label_0 = grouped.median().loc[0]\n",
        "median_label_1 = grouped.median().loc[1]\n",
        "print(\"Median for label 0:\")\n",
        "print(median_label_0)\n",
        "print(\"\\nMedian for label 1:\")\n",
        "print(median_label_1)"
      ],
      "metadata": {
        "id": "6K6ocUEl16yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=100)\n",
        "param_grid = {'n_estimators': [50, 100, 200, 300, 400, 500]}\n",
        "\n",
        "grid = GridSearchCV(rfc, param_grid)\n",
        "grid.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "nf_HS5PT18s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZGpAA6nY0yY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TPG8SYi50x6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZJ9seo6h0xyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NEURAL NETWORK**"
      ],
      "metadata": {
        "id": "PSGJiEf70xom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBpZWQjfPvcd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ2O-mzIq7Q1"
      },
      "outputs": [],
      "source": [
        "#import data\n",
        "df = pd.read_csv(\"processed_data.csv\")\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkPONeZdrtp0"
      },
      "outputs": [],
      "source": [
        "#drop empty\n",
        "df.dropna(how='all', axis=1, inplace=True)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN30Fru1tjjq"
      },
      "outputs": [],
      "source": [
        "#split to features and labels\n",
        "X = df.drop([\"label\"], axis=1)\n",
        "#drop id and unnamed\n",
        "X = X.drop([\"id\"], axis=1)\n",
        "X = X.drop([\"Unnamed: 0\"], axis = 1)\n",
        "y = df[\"label\"]\n",
        "display(X)\n",
        "display(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmhotDpCTEs5"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Identify numeric columns\n",
        "numeric_columns = [\n",
        "    col for col in X.columns\n",
        "    if X[col].dtype in [\"float64\", \"int64\"]\n",
        "]\n",
        "X = X[numeric_columns]\n",
        "\n",
        "#train test split, 80% train\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVJv_yb_UZh_"
      },
      "outputs": [],
      "source": [
        "#feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "display(X_train)\n",
        "display(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccgyjg6-U-B4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDU7XcMdVMXq"
      },
      "outputs": [],
      "source": [
        "class ANN(nn.Module):\n",
        "    def __init__(self, input_dim = 104, output_dim = 1):\n",
        "        super(ANN, self).__init__()\n",
        "        #optimize numbers of neurons and # of layers\n",
        "        # Input Layer 104 (features) -> 64\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        # 64 -> 64\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        # 64 -> 32\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        # 32 -> 32\n",
        "        self.fc4 = nn.Linear(32, 16)\n",
        "        # 32 -> output layer 1\n",
        "        self.output_layer = nn.Linear(16,1)\n",
        "        # Dropout 20% to reduce overfitting\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    # Feed Forward Function\n",
        "    def forward(self, x):\n",
        "        # ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        return nn.Sigmoid()(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWt33Cp5VNYX"
      },
      "outputs": [],
      "source": [
        "# Create NN model\n",
        "model = ANN(input_dim = 104, output_dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ALJmBa3U_Kl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# convert to torch from numpy\n",
        "X_train = torch.from_numpy(X_train)\n",
        "y_train = torch.from_numpy(y_train.to_numpy()).view(-1,1)\n",
        "\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_test = torch.from_numpy(y_test.to_numpy()).view(-1,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmY7AjYKy9_P"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg9jQnnKy-w5"
      },
      "outputs": [],
      "source": [
        "# Make torch datasets from train and test sets\n",
        "train = torch.utils.data.TensorDataset(X_train,y_train)\n",
        "test = torch.utils.data.TensorDataset(X_test,y_test)\n",
        "\n",
        "# Create train and test data loaders\n",
        "# choose batch: 32, 64. 128, 256\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size = 128, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size = 128, shuffle = True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPeHPI4LzEpP"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Binary cross entropy loss that performs the best to predict either 0 or 1\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "# specify learning rate, experiment with it\n",
        "lr = 0.0005\n",
        "# experimentation with multiple optimizers\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9,nesterov = True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay= 1e-6)\n",
        "#scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=1.0, total_iters=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIAuRJgXcmG8"
      },
      "source": [
        "TRAIN AND VALIDATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LvUobExzKAA"
      },
      "outputs": [],
      "source": [
        "# between 10-50\n",
        "epochs = 10\n",
        "\n",
        "epoch_list = []\n",
        "train_loss_list = []\n",
        "train_f1_list = []\n",
        "val_loss_list = []\n",
        "val_f1_list = []\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    # Calculate true positive, false positive, true negative, false negative\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "    tn = 0\n",
        "    fn = 0\n",
        "\n",
        "    # pass in train data\n",
        "    for data,target in train_loader:\n",
        "        data = Variable(data).float()\n",
        "        target = Variable(target).type(torch.FloatTensor)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        predicted = (torch.round(output))\n",
        "\n",
        "        tp += torch.sum((predicted == 1) & (target == 1))\n",
        "        fp += torch.sum((predicted == 1) & (target == 0))\n",
        "        tn += torch.sum((predicted == 0) & (target == 0))\n",
        "        fn += torch.sum((predicted == 0) & (target == 1))\n",
        "\n",
        "        # calculate loss\n",
        "        loss = loss_fn(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()# update running training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "\n",
        " # calculate average training loss over an epoch\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "\n",
        "    precision = tp/(tp + fp)\n",
        "    recall = tp/(tp+fn)\n",
        "    #calculate f1 score for training\n",
        "    f1= (2*precision*recall)/(precision+recall)\n",
        "    #record for visualization\n",
        "    train_f1_list.append(f1)\n",
        "    train_loss_list.append(train_loss)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.4f}'.format(\n",
        "        epoch+1,\n",
        "        train_loss,\n",
        "        ))\n",
        "    print('tp: {}  fp: {}  tn: {}  fn: {}  f1: {:.6f}'.format(\n",
        "        tp,\n",
        "        fp,\n",
        "        tn,\n",
        "        fn,\n",
        "        f1\n",
        "        ))\n",
        "\n",
        "    #validate with test data\n",
        "    with torch.no_grad():\n",
        "      ttp = 0\n",
        "      tfp = 0\n",
        "      ttn = 0\n",
        "      tfn = 0\n",
        "      for data,target in test_loader:\n",
        "\n",
        "        data = Variable(data).float()\n",
        "        target = Variable(target).type(torch.FloatTensor)\n",
        "\n",
        "        output = model(data)\n",
        "        predicted = (torch.round(output))\n",
        "\n",
        "        ttp += torch.sum((predicted == 1) & (target == 1))\n",
        "        tfp += torch.sum((predicted == 1) & (target == 0))\n",
        "        ttn += torch.sum((predicted == 0) & (target == 0))\n",
        "        tfn += torch.sum((predicted == 0) & (target == 1))\n",
        "        loss = loss_fn(output, target)\n",
        "        val_loss += loss.item()*data.size(0)\n",
        "\n",
        "      test_precision = ttp/(ttp + tfp)\n",
        "      test_recall = ttp/(ttp+tfn)\n",
        "      test_f1= (2*test_precision*test_recall)/(test_precision+test_recall)\n",
        "      val_loss = val_loss/len(test_loader.dataset)\n",
        "      val_loss_list.append(val_loss)\n",
        "      val_f1_list.append(test_f1)\n",
        "      print('test f1: {:.6f}'.format(\n",
        "        test_f1\n",
        "        ))\n",
        "      print('Testing Loss: {:.4f}'.format(val_loss))\n",
        "\n",
        "    # Move to next epoch\n",
        "    #scheduler.step() to update lr\n",
        "    epoch_list.append(epoch + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot learning curves to see if it's overfitting with epoch"
      ],
      "metadata": {
        "id": "VygUYt0bCUzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(epoch_list,train_loss_list, label='training')\n",
        "plt.plot(epoch_list, val_loss_list, label='validating')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss vs Number of Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(epoch_list,train_f1_list, label='training')\n",
        "plt.plot(epoch_list, val_f1_list, label='validating')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"F1 Score\")\n",
        "plt.title(\"F1 Scores vs Number of Epochs\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "p2_VqErQCWBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run with test data"
      ],
      "metadata": {
        "id": "qH1-KU27NNe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"processed_test.csv\")\n",
        "ids = test_df[\"id\"]\n",
        "test_df = test_df.drop([\"id\"], axis =1)\n",
        "test_df = test_df.drop([\"Unnamed: 0\"], axis =1)\n",
        "pd.set_option('display.max_columns', None)\n",
        "display(test_df)\n",
        "test_df = sc.fit_transform(test_df)\n",
        "test_df = torch.from_numpy(test_df)\n",
        "test_df = Variable(test_df).float()"
      ],
      "metadata": {
        "id": "ENJRm4nYWz_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to record the predictions\n",
        "results = []\n",
        "model.eval()\n",
        "\n",
        "# run model with test data\n",
        "with torch.no_grad():\n",
        "    for data in test_df:\n",
        "        output = model(data)\n",
        "        pred = int((torch.round(output.data[0])).item())\n",
        "        results.append(pred)"
      ],
      "metadata": {
        "id": "0U0gHbLSbPpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn results into into submission.csv"
      ],
      "metadata": {
        "id": "Sj5TOAtMb-Zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "filename = \"submission.csv\"\n",
        "header = [\"id\", \"predicted\"]\n",
        "ids = ids\n",
        "\n",
        "def turn_to_churn(n):\n",
        "  if 0 == n:\n",
        "    return \"no churn\"\n",
        "  if 1 == n:\n",
        "    return \"churn\"\n",
        "  return \"error\"\n",
        "\n",
        "predicted = map(turn_to_churn, results)\n",
        "\n",
        "rows = [[id, pv] for id, pv in zip(ids, predicted)]\n",
        "# Include the header\n",
        "before_csv = [header] + rows"
      ],
      "metadata": {
        "id": "W0wi-UgiWpBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filename, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Write each row of data to the CSV file\n",
        "    writer.writerows(before_csv)"
      ],
      "metadata": {
        "id": "4kH5kGFVejUz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}